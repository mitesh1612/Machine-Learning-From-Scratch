{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598807399571",
   "display_name": "Python 3.7.3 64-bit ('Continuum': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Code\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, k = 0.5):\n",
    "        self.k = k\n",
    "\n",
    "        self.tokens = set()\n",
    "        self.token_spam_counts = defaultdict(int)\n",
    "        self.token_ham_counts = defaultdict(int)\n",
    "        self.spam_messages = self.ham_messages = 0\n",
    "\n",
    "    def train(self, messages):\n",
    "        for message in messages:\n",
    "            if message.is_spam:\n",
    "                self.spam_messages += 1\n",
    "            else:\n",
    "                self.ham_messages += 1\n",
    "\n",
    "            # Increment word counts\n",
    "            for token in tokenize(message.text):\n",
    "                self.tokens.add(token)\n",
    "                if message.is_spam:\n",
    "                    self.token_spam_counts[token] += 1\n",
    "                else:\n",
    "                    self.token_ham_counts[token] += 1\n",
    "\n",
    "    def _probabilities(self, token):\n",
    "        \"\"\"\n",
    "        Returns P(token|spam) and P(token|ham)\n",
    "        \"\"\"\n",
    "        spam = self.token_spam_counts[token]\n",
    "        ham = self.token_ham_counts[token]\n",
    "\n",
    "        p_token_spam = (spam + self.k) / (self.spam_messages + 2 * self.k)\n",
    "        p_token_ham = (ham + self.k) / (self.ham_messages + 2 * self.k)\n",
    "\n",
    "        return p_token_spam, p_token_ham\n",
    "\n",
    "    def predict(self, text):\n",
    "        text_tokens = tokenize(text)\n",
    "        log_prob_if_spam = log_prob_if_ham = 0.0\n",
    "\n",
    "        # Iterate through each word in vocabulary\n",
    "        for token in self.tokens:\n",
    "            prob_if_spam, prob_if_ham = self._probabilities(token)\n",
    "\n",
    "            # If token appears in message, add the log probability of seeing it\n",
    "            if token in text_tokens:\n",
    "                log_prob_if_spam += math.log(prob_if_spam)\n",
    "                log_prob_if_ham += math.log(prob_if_ham)\n",
    "            # Otherwise add the log probability of not seeing it which is log(1-probability of seeing it)\n",
    "            else:\n",
    "                log_prob_if_spam += math.log(1-prob_if_spam)\n",
    "                log_prob_if_ham += math.log(1-prob_if_ham)\n",
    "        prob_if_spam = math.exp(log_prob_if_spam)\n",
    "        prob_if_ham = math.exp(log_prob_if_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    all_words = re.findall(\"[a-z0-9']+\", text)\n",
    "    return set(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "from collections import namedtuple\n",
    "SpamData = namedtuple(\"SpamData\", ['text','is_spam'])\n",
    "\n",
    "training_data = []\n",
    "\n",
    "with open('SMSSpamCollection') as f:\n",
    "    for line in f:\n",
    "        is_spam = True if line.split()[0] == 'spam' else False\n",
    "        data_point = SpamData(text=' '.join(line.split()[1:]),is_spam=is_spam)\n",
    "        training_data.append(data_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_spam_given_token(token, model):\n",
    "    prob_if_spam, prob_if_ham = model._probabilities(token)\n",
    "\n",
    "    return prob_if_spam/(prob_if_spam + prob_if_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NaiveBayesClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "spammiest_words ['1000', 'tone', 'cs', '500', 'guaranteed', '18', '150p', 'won', 'prize', 'claim']\nhammiest_words ['gt', 'lt', \"i'll\", 'he', 'lor', 'da', 'later', 'she', 'doing', 'ask']\n"
    }
   ],
   "source": [
    "words = sorted(model.tokens, key=lambda t: p_spam_given_token(t,model))\n",
    "print(\"spammiest_words\", words[-10:])\n",
    "print(\"hammiest_words\", words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}